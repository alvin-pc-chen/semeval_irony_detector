{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements, then download and unzip embeddings\n",
    "\n",
    "! pip install -r requirements.txt\n",
    "! wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "! unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.util import load_datasets, split_data\n",
    "\n",
    "# Set up paths and constants\n",
    "embeddings_path = 'glove.twitter.27B.200d.txt'\n",
    "vocab_path = \"./vocab.txt\"\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "\n",
    "# Download and split data\n",
    "train_sentences, train_labels, test_sentences, test_labels, label2i = load_datasets()\n",
    "training_sentences, training_labels, dev_sentences, dev_labels = split_data(train_sentences, train_labels, split=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.tokenizer import Tokenizer\n",
    "\n",
    "# Set up tokenizer and make vocab\n",
    "tokenizer = Tokenizer()\n",
    "all_data = train_sentences + test_sentences\n",
    "tokenized_data = tokenizer.tokenize(all_data)\n",
    "vocab = sorted(set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws]))\n",
    "with open('vocab.txt', 'w') as vf:\n",
    "    vf.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.embeddings import read_pretrained_embeddings, get_oovs, update_embeddings\n",
    "\n",
    "# Load the pretrained embeddings, find the out-of-vocabularies, and add to word2i and embeddings\n",
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(embeddings_path, vocab_path)\n",
    "oovs = get_oovs(vocab_path, glove_word2i)\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.ironymodel import IronyDetector\n",
    "\n",
    "# Initialize model\n",
    "model = IronyDetector(\n",
    "    input_dim=embeddings.shape[1],\n",
    "    hidden_dim=128,\n",
    "    embeddings_tensor=embeddings,\n",
    "    pad_idx=word2i[\"<PAD>\"],\n",
    "    output_size=len(label2i),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.processing import make_batches, encode_sentences, encode_labels\n",
    "import torch\n",
    "\n",
    "# Set hyperparameters\n",
    "batch_size = 8\n",
    "epochs = 3\n",
    "learning_rate = 0.00005\n",
    "weight_decay = 0\n",
    "optimizer = torch.optim.AdamW(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Create batches\n",
    "batch_tokenized = []\n",
    "for batch in make_batches(training_sentences, batch_size):\n",
    "    batch_tokenized.append(tokenizer(batch))\n",
    "batch_labels = make_batches(training_labels, batch_size)\n",
    "dev_sentences = tokenizer(dev_sentences)\n",
    "test_sentences = tokenizer(test_sentences)\n",
    "\n",
    "# Encode data\n",
    "train_features = [encode_sentences(batch, word2i) for batch in batch_tokenized]\n",
    "train_labels = [encode_labels(batch) for batch in batch_labels]\n",
    "dev_features = encode_sentences(dev_sentences, word2i)\n",
    "dev_labels = [int(l) for l in dev_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.ironymodel import training_loop\n",
    "\n",
    "# Train model\n",
    "trained_model = training_loop(\n",
    "    epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_features,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    label2i,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.ironymodel import predict\n",
    "from functions.util import f1_score, avg_f1_score\n",
    "\n",
    "# Test model\n",
    "test_features = encode_sentences(test_sentences, word2i)\n",
    "test_labels = [int(l) for l in test_labels]\n",
    "preds = predict(trained_model, test_features)\n",
    "dev_f1 = f1_score(preds, test_labels, label2i['1'])\n",
    "dev_avg_f1 = avg_f1_score(preds, test_labels, set(label2i.values()))\n",
    "print(f\"Test F1 {dev_f1}\")\n",
    "print(f\"Avg Test F1 {dev_avg_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
